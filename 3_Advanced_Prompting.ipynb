{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe75baa",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a8ddf",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
    "\n",
    "## 1. Introduction: The Inner Monologue\n",
    "\n",
    "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
    "\n",
    "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering. \n",
    "\n",
    "### Why use a \"Dumb\" Model?\n",
    "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
    "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
    "\n",
    "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
    "\n",
    "### Visualizing the Process (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Input[Question: 5+5*2?]\n",
    "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
    "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
    "    Step1 --> Step2[Step 2: 5+10=15]\n",
    "    Step2 --> Correct[Answer: 15 (Correct)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41e5ba",
   "metadata": {},
   "source": [
    "## 2. Concept: Latent Reasoning\n",
    "\n",
    "Why does this work?\n",
    "Because LLMs are \"Next Token Predictors\".\n",
    "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
    "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
    "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
    "\n",
    "**Writing is Thinking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba92b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Groq API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3088780",
   "metadata": {},
   "source": [
    "## 3. The Experiment: A Tricky Math Problem\n",
    "\n",
    "Let's try a problem that requires multi-step logic.\n",
    "\n",
    "**Problem:**\n",
    "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a70d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
      "\n",
      "2 cans * 3 tennis balls per can = 6 tennis balls\n",
      "\n",
      "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
      "\n",
      "5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
    "\n",
    "# 1. Standard Prompt (Direct Answer)\n",
    "prompt_standard = f\"Answer this question: {question}\"\n",
    "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_standard).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b696ba6",
   "metadata": {},
   "source": [
    "### Critique\n",
    "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
    "\n",
    "Let's force it to think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd65b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain of Thought (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
      "\n",
      "1. Roger already has 5 tennis balls.\n",
      "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
      "3. Now, we add the tennis balls he already had (5) to the new tennis balls he bought (6). 5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "# 2. CoT Prompt (Magic Phrase)\n",
    "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
    "\n",
    "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_cot).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205672",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "Look at the output. By explicitly breaking it down:\n",
    "1.  \"Roger starts with 5.\"\n",
    "2.  \"2 cans * 3 balls = 6 balls.\"\n",
    "3.  \"5 + 6 = 11.\"\n",
    "\n",
    "The model effectively \"debugs\" its own logic by generating the intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee779f",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1fa7c",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
    "\n",
    "## 1. Introduction: Beyond A -> B\n",
    "\n",
    "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
    "\n",
    "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a348d6",
   "metadata": {},
   "source": [
    "## 2. Tree of Thoughts (ToT)\n",
    "\n",
    "ToT explores multiple branches before making a decision. \n",
    "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
    "\n",
    "### Implementation\n",
    "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea2d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tree of Thoughts (ToT) Result ---\n",
      "Based on the three proposed solutions, I would recommend **Solution 2: \"Garden-to-Table Adventure\"** as the most sustainable and effective approach to encourage a 5-year-old to eat vegetables. Here's why:\n",
      "\n",
      "1. **Hands-on learning**: This approach involves children in the process of growing, harvesting, and cooking vegetables, which helps them develop a deeper understanding of where their food comes from and the effort that goes into producing it.\n",
      "2. **Sense of responsibility**: By involving children in the care of the garden, they develop a sense of responsibility for the food they grow and harvest. This builds a connection between the child and the food, making them more invested in trying and eating the vegetables.\n",
      "3. **Development of healthy habits**: This approach encourages children to adopt healthy eating habits from a young age, which is essential for their overall well-being and development.\n",
      "4. **Long-term impact**: The \"Garden-to-Table Adventure\" approach has a long-term impact, as children learn to appreciate the value of fresh, homegrown produce and develop a lifelong love for healthy eating.\n",
      "5. **Avoids bribery**: Unlike the other two solutions, which may involve rewards or incentives for eating vegetables, the \"Garden-to-Table Adventure\" approach encourages children to develop a genuine interest in healthy eating without relying on rewards or bribes.\n",
      "\n",
      "As a child psychologist, I would recommend this approach because it:\n",
      "\n",
      "* Encourages children to develop a sense of agency and control over their food choices\n",
      "* Fosters a positive relationship with food and the natural world\n",
      "* Promotes healthy eating habits and a balanced diet\n",
      "* Provides opportunities for hands-on learning and exploration\n",
      "* Develops critical thinking and problem-solving skills\n",
      "\n",
      "Overall, the \"Garden-to-Table Adventure\" approach is a sustainable and effective way to encourage a 5-year-old to eat vegetables, as it promotes healthy habits, develops a sense of responsibility, and fosters a positive relationship with food and the natural world.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
    "\n",
    "# Step 1: The Branch Generator\n",
    "prompt_branch = ChatPromptTemplate.from_template(\n",
    "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
    ")\n",
    "\n",
    "branches = RunnableParallel(\n",
    "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
    "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
    "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Step 2: The Judge\n",
    "prompt_judge = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three proposed solutions for: '{problem}'\n",
    "    \n",
    "    1: {sol1}\n",
    "    2: {sol2}\n",
    "    3: {sol3}\n",
    "    \n",
    "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Chain: Input -> Branches -> Judge -> Output\n",
    "tot_chain = (\n",
    "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
    "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]}) \n",
    "    | prompt_judge\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
    "print(tot_chain.invoke(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38579cab",
   "metadata": {},
   "source": [
    "## 3. Graph of Thoughts (GoT)\n",
    "\n",
    "You asked: **\"Where is Graph of Thoughts?\"**\n",
    "\n",
    "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
    "\n",
    "### The Workflow (Writer's Room)\n",
    "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
    "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
    "3.  **Refine:** Polish the Master Plot.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "   Start(Concept) --> A[Draft 1]\n",
    "   Start --> B[Draft 2]\n",
    "   Start --> C[Draft 3]\n",
    "   A & B & C --> Mixer[Aggregator]\n",
    "   Mixer --> Final[Final Story]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Graph of Thoughts (GoT) Result ---\n",
      "\"Echoes of Eternity\" is a mind-bending, heart-pumping thriller that combines the thrill of time travel with the terror of the unknown. Dr. Sophia Ellis, a brilliant physicist, has discovered a way to harness the power of quantum entanglement to traverse the space-time continuum, but her actions inadvertently unleash a malevolent force that begins to manipulate and hunt down her past and future selves across multiple timelines. As Sophia navigates a labyrinthine web of alternate realities, she finds herself reliving a passionate and all-consuming love affair with a man from her past, but at a terrible cost: with each iteration, her loved one begins to change, becoming increasingly sinister and menacing, threatening to destroy the very fabric of their love and the world itself. With time running out, Sophia must confront the darkness within herself and the true nature of her technology, all while racing against the clock to restore balance to the timestream and save the people she loves from the clutches of a monstrous, otherworldly presence that will stop at nothing to claim her soul.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Generator (Divergence)\n",
    "prompt_draft = ChatPromptTemplate.from_template(\n",
    "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
    ")\n",
    "\n",
    "drafts = RunnableParallel(\n",
    "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
    "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
    "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# 2. The Aggregator (Convergence)\n",
    "prompt_combine = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three movie ideas for the topic '{topic}':\n",
    "    1. Sci-Fi: {draft_scifi}\n",
    "    2. Romance: {draft_romance}\n",
    "    3. Horror: {draft_horror}\n",
    "    \n",
    "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
    "    Write one paragraph.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. The Chain\n",
    "got_chain = (\n",
    "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
    "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]}) \n",
    "    | prompt_combine\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
    "print(got_chain.invoke(\"Time Travel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cb724",
   "metadata": {},
   "source": [
    "## 4. Summary & Comparison Table\n",
    "\n",
    "| Method | Structure | Best For... | Cost/Latency |\n",
    "|--------|-----------|-------------|--------------|\n",
    "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
    "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
    "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High | \n",
    "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
    "\n",
    "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f914f2-7757-49b8-a4fc-ece6f78a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chain of Thought prompting encourages the model to think step by step.\n",
    "This improves reasoning and gives more accurate answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a1e28-bc5e-4aa9-827e-120610072330",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree of Thoughts explores multiple solutions and selects the best one.\n",
    "It improves decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf8afa-d361-4a20-909e-16d988373e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph of Thoughts combines multiple ideas into a final output.\n",
    "It is useful for creative and complex tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
